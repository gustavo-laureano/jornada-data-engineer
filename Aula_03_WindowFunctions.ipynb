{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05c27d13-06b2-4319-b7ed-31efa96e4142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Criar um DataFrame manual que tenha o mesmo ID de venda repetido 3\n",
    "vezes, mudando apenas a coluna data_atualizacao\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef057269-b1b5-4fe0-9578-aed2e22d72bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dados_repetidos = [\n",
    "  (101, \"Camisa\", 50.0, \"2026-02-02 00:01:03\"), \n",
    "  (101, \"Camisa\", 50.0, \"2026-02-02 00:01:04\"), \n",
    "  (101, \"Camisa\", 50.0, \"2026-02-02 00:01:05\")  \n",
    "]\n",
    "\n",
    "colunas = [\"id_venda\", \"produto\", \"valor\", \"data_atualizacao\"]\n",
    "\n",
    "df_repeated = spark.createDataFrame(dados_repetidos, colunas)\n",
    "\n",
    "display(df_repeated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cd10192-703c-449c-887d-4eb352a40ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7401e16-6f7d-424f-ad16-661960b1b0ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc, col \n",
    "\n",
    "# Criar uma janela particionada pelo id_venda e ordenada pela data_atualizacao decrescente.\n",
    "window = Window.partitionBy(\"id_venda\").orderBy(desc(\"data_atualizacao\"))\n",
    "\n",
    "# Criar uma coluna rn (row number) usando essa janela.\n",
    "df_repeated = df_repeated.withColumn(\"rn\", row_number().over(window))\n",
    "\n",
    "# Criar uma janela particionada pelo id_venda e ordenada\n",
    "window = Window.partitionBy(\"id_venda\").orderBy(col(\"rn\"))\n",
    "\n",
    "# Filtrar apenas onde rn 1 (a linha mais recente).\n",
    "df_repeated = df_repeated.where(col(\"rn\") == 1)\n",
    "display(df_repeated)\n",
    "\n",
    "# Explicar no código (comentário) por que usamos row_number e não distinct .\n",
    "# O distinct() não segue regra de qual excluir, ele remove linhas idênticas ou escolhe uma aleatória. O row_number() permite que você decida qual linha é a vencedora baseada em uma lógica como no exemplo a data mais recente de atualização do cadastro."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aula_03_WindowFunctions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

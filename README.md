# üöÄ Jornada Data Engineer

Este reposit√≥rio cont√©m o registro da minha evolu√ß√£o na trilha de Engenharia de Dados, utilizando a arquitetura **Lakehouse** com **Databricks** e **PySpark** guiada por [@nicolasn892](https://github.com/nicolasn892). O objetivo √© simular cen√°rios reais de ingest√£o, transforma√ß√£o e qualidade de dados.

## üõ†Ô∏è Tecnologias Utilizadas
* **Databricks** (Community Edition)
* **PySpark** (Spark SQL & DataFrames)
* **GitHub** para versionamento e portf√≥lio

---

## üéØ Miss√µes Conclu√≠das

### Miss√£o 01: Inicia√ß√£o no Lakehouse
* **Objetivo:** Configurar o ambiente de engenharia e rodar o primeiro pipeline de dados.
* **O que foi feito:**
    * Simula√ß√£o da camada **Bronze** com dados manuais.
    * Transforma√ß√£o para a camada **Silver** com limpeza de nulos e l√≥gica de aumento salarial (10%) para o departamento de TI.
    * Cria√ß√£o de uma regra de exce√ß√£o: departamento "Gest√£o" n√£o recebe aumento.
    * Agrega√ß√£o final (camada **Gold**) usando SQL para calcular m√©dia salarial por departamento.
* **Arquivo:** `Aula_01_Engenharia.ipynb`

### Miss√£o 02: Integrar GitHub
* **Objetivo:** Integrar o Databricks ao GitHub para salvar e versionar o c√≥digo.
* **O que foi feito:**
    * Configura√ß√£o de **Personal Access Tokens (Classic)** no GitHub com permiss√£o de `repo`.
    * Conex√£o via **Git Integration** nas configura√ß√µes do Databricks.
    * Realiza√ß√£o do primeiro *Commit & Push* do projeto diretamente do workspace.

### Miss√£o 03: A Realidade dos Arquivos (Ingest√£o)
* **Objetivo:** Aprender a lidar com arquivos f√≠sicos (CSV) e sistema de arquivos DBFS.
* **O que foi feito:**
    * Upload de arquivo CSV externo para o **Data Lake** (DBFS) do Databricks.
    * Leitura dos dados usando `spark.read.format("csv")` com cabe√ßalho.
    * Adi√ß√£o de coluna `data_processamento` com o timestamp atual ajustado para o fuso hor√°rio local.
    * Escrita dos dados transformados em formato **Parquet** na pasta de sa√≠da `sales_silver`.
* **Arquivo:** `Aula_02_Ingestao.ipynb`

### Miss√£o 04: O Pesadelo da Duplicidade (Quality)
* **Objetivo:** Resolver problemas de dados duplicados utilizando **Window Functions**.
* **O que foi feito:**
    * Cria√ß√£o de um DataFrame manual com IDs de venda repetidos e diferentes datas de atualiza√ß√£o.
    * Implementa√ß√£o de uma janela (`Window`) particionada por `id_venda` e ordenada por `data_atualizacao` decrescente.
    * Uso da fun√ß√£o `row_number()` para identificar a linha mais recente de cada venda.
    * Filtragem final para manter apenas os registros onde `rn == 1`.
* **Arquivo:** `Aula_03_WindowFunctions.ipynb`

---

## üß† Aprendizados Principais
* **Arquitetura Medallion:** Entendimento pr√°tico das camadas Bronze (dados brutos), Silver (limpeza e regras de neg√≥cio) e Gold (agrega√ß√£o para an√°lise).
* **Qualidade de Dados:** Diferen√ßa fundamental entre usar `distinct()` (que remove linhas id√™nticas) e `row_number()` (que permite aplicar crit√©rios l√≥gicos para decidir qual registro √© o "vencedor").
